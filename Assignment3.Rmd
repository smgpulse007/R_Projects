---
title: "Assignment III"
author: "Shailesh Dudala"
date: "February 5, 2019"
output: html_document
---


> Please submit your answers by 5:59 pm on Feb 11, 2019. Remember to show your work. In other words, always use echo=TRUE for the R code chunks that you provide. NOTE - All plots must show proper title, axis lables, and any legends used. Points will be deducted otherwise.  


## Question 1: Simple Linear Regression
We are going to work with the dataset bike_data.csv (provided in Files->Assignments->Assignment_3). This dataset has been dowloaded from Kaggle, which is an online prediction contest website (see https://www.kaggle.com/c/bike-sharing-demand/data). The data is essentially the log of hourly bike rentals in a city over two years. The following is the codebook:

. datetime - hourly date + timestamp      
. season -  1 = spring, 2 = summer, 3 = fall, 4 = summer      
. holiday - whether the day is considered a holiday     
. workingday - whether the day is neither a weekend nor holiday     
. weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy , 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist , 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds , 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog        
. temp - temperature in Celsius         
. atemp - "feels like" temperature in Celsius       
. humidity - relative humidity        
. windspeed - wind speed      .
. casual - number of non-registered user rentals initiated        
. registered - number of registered user rentals initiated      
. count - number of total rentals


First, we need to do some preprocessing. Specifically, we need to process the year variable and remove all observations with weather == 4 (these are outliers and need to be be removed). 


```{r, echo = FALSE, message = FALSE, warning=FALSE}
# set up
rm(list=ls())
library(plyr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(readr)
library(weathermetrics)
library(tidyr)

# Read the dataset in
d.in <- read_csv("bike_data.csv")

# Preprocess
d.in <- d.in %>% mutate(datetime_f = mdy_hm(datetime)) %>%
  mutate(year = as.factor(year(datetime_f))) %>%
  mutate(weather = as.factor(weather)) %>%
  filter(weather != "4")


```

(a) Perform the following simple linear regression: count ~ temperature. What are the coefficients and their 95% confidence intervals?        
Ans. 

The Intercept is 6 and the coeffiencent of tempererature is 9.172048
The confint is has range of (8.770590, 9.573505) at 95% C.I., which means for a 1 degree(C) increase in temperature, there would be an increase of the number of rentals between range of (8.77, 9.57).
```{r}
## Insert code below
slr <- lm(count ~ temp, d.in)
#summary(slr)
coef(slr)
confint(slr)

```

(b) Interpret your results in terms of increase or decrease in temperature. Mention specifically what is the  meaning of the intercept and the direction and magnitude of the association between temperature and count.
Ans.  

For a 1 degree celsius increase in temperature (temp), the number of bike rentals (count) increases by 9.17. The slope is positive which shows a positive colrealation between the count and temperature, i.e, if temp increases, count increases. 

We can see below that for a unit increase in the temperature, the number of bike rentals (count) increase by a factor that is equal to the slope of the linear model (9.172048).
```{r}
test_temp <- data.frame(temp = c(0,1,2,3,4,5))
#View(test_temp)
predict(slr, test_temp)
```




(c) Using mutate, create a new variable temp_f which is Farenheit representation of values in the temp variable. Perform the regression count ~ temp_f and interpret the results. What conclusions can you draw from this experiment?      
Ans. 

You cannot draw any conclusions from the intercept since the -157 rentals is not possible. An increase in 1 degree F is associated with an increase of 5.09 bike rentals.
```{r}
## Insert code below
d.in <- d.in %>% mutate(temp_f = celsius.to.fahrenheit(temp))
slr_f <- lm(count ~ temp_f, d.in)
coef(slr_f)
confint(slr_f)

#ggplot(d.in, aes(temp_f, count)) + geom_jitter(aes(color = temp_f), alpha = 0.4) + scale_color_gradient(low = "green", high = "red") + ggtitle("Temperature(F) vs Count")
```


## Question 2: Multiple Linear Regression - I
On the same datasetas Q1, perform the following multiple linear regression: count ~ temp + season + humidity + weather + year. Keep season and weather as categorical variables. Interpret your results through the following means :

(a) what is the intercept and what does it mean? 
Ans. 

The intercept is 98.5 ~ 98 on average and it means that if there is no relation between the bike rentals and the predictor variables(temp, season, humidity, weather and year), then the number of bike rented would be 98.
```{r}
## Insert code below
#Convert season to a categorical variable (factor)
d.in <- d.in %>% mutate(season = as.factor(season))
#weather is already a factor in the data frame
mlr <- lm(count ~ temp + season + humidity + weather + year, d.in)
summary(mlr)
#confint(mlr)
```


(b) how does each variable contribute to count in terms of increase or decrease?      
Ans.

temp : If all other predictor variables are held constant, for a unit increase in temperature (1 degree C), the total number of bike rentals (count) increases by about 10(10.43).

season2 : If all other predictor variables are held constant, for a change in season from spring to summer, the total number of bike rentals (count) increases by about 4(4.71).

season3 : If all other predictor variables are held constant, for a change in season from spring to fall, the total number of bike rentals (count) decreases by about 29(-29.1).

humidity : If all other predictor variables are held constant, for a unit increase in humidity (1% increase), the total number of bike rentals (count) decreases by about 2(-2.73).

weather2 : If all other predictor variables are held constant, for a change in weather from "Clear, Few clouds, Partly cloudy, Partly cloudy" to "Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds", the total number of bike rentals (count) increases by about 11(11.34).

weather3 : If all other predictor variables are held constant, for a change in weather from "Clear, Few clouds, Partly cloudy, Partly cloudy" to "Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds" , the total number of bike rentals (count) decreases by about 7(7.37).

year2012 : If all other predictor variables are held constant, for a change in year from 2011 to 2012, the total number of bike rentals (count) increases by about 75(75.87).



(c) what can you say about the results and the quality of fit? Use pvalue threshold of < 0.001 to reject any null hypothesis.     
Ans. 
The null hypothesis is that there is no correlation between the number of bikes rented and the predictor varibales in the dataset.
The alternate hypothesis is that there actually is an association between the number of bikes rented out and each predictor variable.

The Null Hypothesis could be accepted for "season2", "weather2" and "weather3" predictor variables as the p-values for those three are >0.001.
```{r}
pval_S2 <- summary(mlr)$coefficient[,"Pr(>|t|)"][3]
pval_W2 <- summary(mlr)$coefficient[,"Pr(>|t|)"][7]
pval_W3 <- summary(mlr)$coefficient[,"Pr(>|t|)"][8]
cat("The association between bike rentals and change of season(season=2) is insignificant as the p-value is", pval_S2,"(>0.001)", "\n")
cat("The association between bike rentals and change of weather(weather=2) is insignificant as the p-value is", pval_W2,"(>0.001)", "\n")
cat("The association between bike rentals and change of weather(weather=3) is insignificant as the p-value is", pval_W3, "(>0.001)", "\n")
```

Whereas for the rest of the predictor variables the Null Hypothesis could be rejected and the Alternate Hypthesis could be accepted since all their p-values are statistically significant. (<0.001)
```{r}

pval_t <- summary(mlr)$coefficient[,"Pr(>|t|)"][2]
pval_S3 <- summary(mlr)$coefficient[,"Pr(>|t|)"][4]
pval_S4 <- summary(mlr)$coefficient[,"Pr(>|t|)"][5]
pval_Hum <- summary(mlr)$coefficient[,"Pr(>|t|)"][6]
pval_Y <- summary(mlr)$coefficient[,"Pr(>|t|)"][9]
cat("The association between bike rentals and change in temp is statistically significant as the p-value is", pval_t,"(<0.001)", "\n")
cat("The association between bike rentals and change of season(season=3) is statistically significant as the p-value is", pval_S3,"(<0.001)", "\n")
cat("The association between bike rentals and change of season(season=4) is statistically significant as the p-value is", pval_S4, "(<0.001)", "\n")
cat("The association between bike rentals and change in humidity is statistically significant as the p-value is", pval_S4, "(<0.001)", "\n")
cat("The association between bike rentals and change in the year is statistically significant as the p-value is", pval_Y, "(<0.001)", "\n")
```



```{r}
mlr.res <- resid(mlr)
plot(mlr.res)
abline(0,0)
err_pc <- sigma(mlr)/mean(d.in$count)
cat("The RSE of 149.6 corresponds to an error percentage of", err_pc*100,"%", "\n")
cat("The implies that the quality of the fit is pretty poor as the RSE is too high and so is the corresponding error percentage")
cat("Also, since the R-squared statistic is about 0.3, only around 30% of the total variance is explained.")

```



## Question 3: Multiple Linear Regression - II
This question deals within application of linear regression. Download the dataset titled "sales_advertising.csv" from Files -> Assignments -> Assignment_3. The dataset measure sales of a product as a function of advertising budgets for TV, radio, and newspaper media. The following is the data dictionary.    

(1) TV: advertising budget for TV (in thousands of dollars)  
(2) radio: advertising budget for radio (in thousands of dollars)  
(3) newspaper:  advertising budget for newspaper (in thousands of dollars)  
(4) sales: sales of product (in thousands of units)   

(a) Plot the response (sales) against all three predictors in three separate plots. Write your code below. Do any of the plots show a linear trend?      
Ans. 

The TV plot does show a relatively stronger linear trend compared to the Radio plot which in turns shows a much stronger trend than the Newspaper plot.
However, none of the plots truly fit a linear model without residual errors as we can see from the regression lines below.
```{r}
## Insert code below
d.sales <- read_csv("sales_advertising.csv")
#View(d.sales)
perf1 <- performance(pred1,"tpr","fpr")
#95% C.I. default 


```


(b) Perform a simple regression to model sales ~ TV. Write your code below. What is the observed association between sales and TV? What is the null hypothesis for this particular model? From the regression, what can we say about the null hypothesis? Use a p-value threshold of <0.05 to indicate significance.        
Ans. 

For a unit increase in the advertising budget(or if the advertising budget is increased by $1000), then the number of TV units sold would see and expected increase of 47. (0.047537*1000)
```{r}
s.lm <- lm(Sales ~ TV, d.sales)
#summary(s.lm)
summary(s.lm)$coefficient
```

The null hypothesis is that there is no correaltion between the sale of product (in 1000s of units) and the budget spent on its advertising through TV (in $1000).
However, we can safely reject the null hypothesis because :
```{r}
spval_TV <- summary(s.lm)$coefficient[,"Pr(>|t|)"][2]
cat("The association between sales and TV is significant as the p-value is", spval_TV,"(<0.05)")
```


(c) Perform a simple regression to model sales ~ newspaper. Write your code below. What is the observed association between sales and newspaper? What is the null hypothesis for this particular model? From the regression, what can we say about the null hypothesis? Use a p-value threshold of <0.05 to indicate significance.        
Ans. 
For a unit increase in the advertising budget(or if the advertising budget is increased by $1000), then the number of TV units sold would see and expected increase of 54. (0.0546931*1000)
```{r}
s2.lm <- lm(Sales ~ Newspaper, d.sales)
#summary(s2.lm)
summary(s2.lm)$coefficient
```

The null hypothesis is that there is no correaltion between the sale of product (in 1000s of units) and the budget spent on its advertising through Newspaper (in $1000).
However, we can safely reject the null hypothesis because :
```{r}
spval_NP <- summary(s2.lm)$coefficient[,"Pr(>|t|)"][2]
cat("The association between sales and TV is significant as the p-value is", spval_NP,"(<0.05)")
```

(d)  Perform a multiple linear regression to model sales ~ TV + radio + newspaper.      
Ans.
```{r}
m.lm <- lm(Sales ~ TV + Radio + Newspaper, d.sales)
summary(m.lm)$coefficient
confint(m.lm)

```
i.  What are the observed associations between sales and each of the media budgets? Mention which associations are significant. Use a p-value threshold of <0.05 to indicate significance.      
Ans. 
```{r}
pval_TV <- summary(m.lm)$coefficient[,"Pr(>|t|)"][2]
pval_R  <- summary(m.lm)$coefficient[,"Pr(>|t|)"][3]
pval_NP <- summary(m.lm)$coefficient[,"Pr(>|t|)"][4]
cat("The association between sales and TV is statistically significant as the p-value is", pval_TV, "\n")
cat("The association between sales and Radio is statistically significant as the p-value is", pval_R, "\n")
cat("The association between sales and Newspaper is statistically insignificant as the p-value is", pval_NP, "\n")
```

ii. Do you observe any difference in the associations in the multiple regression model vs. the simple regression model? Explain why or why not.     
Ans. 
The main difference between the multiple regression and the linear regression model is that the p-value for the Newspaper predictor variable was significant in the linear regression model but not in the multiple regression model. 

So this implies that Newspapers advertising budget didn't really play role in increasing the sales of the product in the multiple regression model, however, it did play a role in the linear regression model.

Also, the R-Squared statistics shows that .89 of the variance is well explained, the F-statistic p-value is less than .05, which means the model is statistically significant. The F-statisticS, corrects the low p-value seen in simple linear regresson. 

